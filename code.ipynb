{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nderwoodfrank/Prediction-of-Sparse-Network/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4wR8cqd-T-m"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/TimDettmers/sparse_learning.git\n",
        "%cd sparse_learning\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqzKZ9x3DUzh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCEiz-_WdawK"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch.optim.lr_scheduler')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3WyZzTM_rUi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "import argparse\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.datasets import CIFAR10, CIFAR100, MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import sparselearning\n",
        "from sparselearning.core import add_sparse_args, CosineDecay, Masking\n",
        "from sparselearning.funcs import no_redistribution, magnitude_prune, random_growth\n",
        "import time\n",
        "import math\n",
        "from math import ceil\n",
        "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
        "import logging\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW796Oxp_vTD"
      },
      "outputs": [],
      "source": [
        "def setup_logger():\n",
        "    logger = logging.getLogger()  # Get the root logger\n",
        "    if not logger.handlers:  # Check if the logger already has handlers\n",
        "        logger.setLevel(logging.INFO)\n",
        "\n",
        "        # Create formatter\n",
        "        formatter = logging.Formatter(fmt='%(asctime)s: %(message)s', datefmt='%H:%M:%S')\n",
        "\n",
        "        # Create file handler which logs even debug messages\n",
        "        log_path = './logs/training.log'\n",
        "        if not os.path.exists('./logs'):\n",
        "            os.mkdir('./logs')\n",
        "        fh = logging.FileHandler(log_path)\n",
        "        fh.setLevel(logging.INFO)\n",
        "        fh.setFormatter(formatter)\n",
        "        logger.addHandler(fh)\n",
        "\n",
        "        # Create console handler with a higher log level\n",
        "        ch = logging.StreamHandler()\n",
        "        ch.setLevel(logging.INFO)\n",
        "        ch.setFormatter(formatter)\n",
        "        logger.addHandler(ch)\n",
        "\n",
        "    return logger\n",
        "logger = setup_logger()\n",
        "log_interval = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLGk8TIR_yAn"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet34(c=10):\n",
        "    model=ResNet(BasicBlock, [3,4,6,3],c)\n",
        "    return model\n",
        "\n",
        "def ResNet50(c=100):\n",
        "    model=ResNet(Bottleneck, [3,4,6,3],c)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpVhn56J_2W1"
      },
      "outputs": [],
      "source": [
        "VGG_CONFIGS = {\n",
        "    # M for MaxPool, Number for channels\n",
        "    'like': [\n",
        "        64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M',\n",
        "        512, 512, 512, 'M'\n",
        "    ],\n",
        "    'D': [\n",
        "        64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M',\n",
        "        512, 512, 512, 'M'\n",
        "    ],\n",
        "    'C': [\n",
        "        64, 64, 'M', 128, 128, 'M', 256, 256, (1, 256), 'M', 512, 512, (1, 512), 'M',\n",
        "        512, 512, (1, 512), 'M' # tuples indicate (kernel size, output channels)\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "\n",
        "    def __init__(self, config, num_classes=10, save_features=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = self.make_layers(VGG_CONFIGS[config], batch_norm=True)\n",
        "        self.feats = []\n",
        "        self.densities = []\n",
        "        self.save_features = save_features\n",
        "\n",
        "\n",
        "        if config == 'C' or config == 'D':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear((512 if config == 'D' else 2048), 512),  # 512 * 7 * 7 in the original VGG\n",
        "                nn.ReLU(True),\n",
        "                nn.BatchNorm1d(512),  # instead of dropout\n",
        "                nn.Linear(512, 512),\n",
        "                nn.ReLU(True),\n",
        "                nn.BatchNorm1d(512),  # instead of dropout\n",
        "                nn.Linear(512, num_classes),\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512, 512),  # 512 * 7 * 7 in the original VGG\n",
        "                nn.ReLU(True),\n",
        "                nn.BatchNorm1d(512),  # instead of dropout\n",
        "                nn.Linear(512, num_classes),\n",
        "            )\n",
        "\n",
        "    @staticmethod\n",
        "    def make_layers(config, batch_norm=False):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for v in config:\n",
        "            if v == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                kernel_size = 3\n",
        "                if isinstance(v, tuple):\n",
        "                    kernel_size, v = v\n",
        "                conv2d = nn.Conv2d(in_channels, v, kernel_size=kernel_size, padding=1)\n",
        "                if batch_norm:\n",
        "                    layers += [\n",
        "                        conv2d,\n",
        "                        nn.BatchNorm2d(v),\n",
        "                        nn.ReLU(inplace=True)\n",
        "                    ]\n",
        "                else:\n",
        "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "                in_channels = v\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer_id, layer in enumerate(self.features):\n",
        "            x = layer(x)\n",
        "\n",
        "            if self.save_features:\n",
        "                if isinstance(layer, nn.ReLU):\n",
        "                    self.feats.append(x.clone().detach())\n",
        "                    self.densities.append((x.data != 0.0).sum().item()/x.numel())\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_GzDuUD_4-2"
      },
      "outputs": [],
      "source": [
        "def get_transforms(dataset_name):\n",
        "    if dataset_name == 'cifar10':\n",
        "        normalize = transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                         (0.2023, 0.1994, 0.2010))\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: F.pad(x.unsqueeze(0), (4, 4, 4, 4), mode='reflect').squeeze()),\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomCrop(32),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "\n",
        "    return transform_train, transform_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6FSQ9Kh_7Ui"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_name, batch_size):\n",
        "    transform_train, transform_test = get_transforms(dataset_name)\n",
        "\n",
        "    if dataset_name == 'cifar10':\n",
        "        full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "        train_size = int(0.8 * len(full_trainset))\n",
        "        validate_size = len(full_trainset) - train_size\n",
        "        trainset, validateset = random_split(full_trainset, [train_size, validate_size])\n",
        "\n",
        "        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    validate_loader = DataLoader(validateset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, validate_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d14IFegI_87Z"
      },
      "outputs": [],
      "source": [
        "class SelectivePruning:\n",
        "    def __init__(self, model, optimizer, density, prune_rate, sparsity_ratio):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.density = density\n",
        "        self.prune_rate = prune_rate\n",
        "        self.sparsity_ratio = sparsity_ratio\n",
        "        self.logger = logging.getLogger(\"SelectivePruning\")\n",
        "\n",
        "        self.masking = Masking(optimizer, CosineDecay(prune_rate, 100), prune_rate, growth_mode='random', prune_mode='magnitude', redistribution_mode='none', verbose=True)\n",
        "        self.masking.add_module(model, density)\n",
        "\n",
        "        self.selected_block_name = self.select_block_for_pruning()\n",
        "        self.logger.info(f\"Selected block for focused pruning and growth: {self.selected_block_name}\")\n",
        "\n",
        "    def apply_masks(self):\n",
        "        \"\"\"Apply masks to enforce sparsity after each optimizer step.\"\"\"\n",
        "        self.masking.apply_mask()\n",
        "\n",
        "    def calculate_total_magnitude(self, module):\n",
        "        \"\"\"Calculate the total magnitude of weights in a module.\"\"\"\n",
        "        return sum(p.data.abs().sum().item() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "    def select_block_for_pruning(self):\n",
        "        \"\"\"Select a block that is neither a shortcut in ResNet nor the first/last layer generally.\"\"\"\n",
        "        block_magnitudes = {}\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.Linear)) and \"shortcut\" not in name:\n",
        "               if not name.startswith(('conv1', 'bn1')) and not name.endswith(('conv3', 'bn3')):\n",
        "                  magnitude = self.calculate_total_magnitude(module)\n",
        "                  block_magnitudes[name] = magnitude\n",
        "\n",
        "        selected_block = max(block_magnitudes, key=block_magnitudes.get, default=None)\n",
        "        self.logger.info(f\"Eligible blocks for pruning: {list(block_magnitudes.keys())}\")\n",
        "        return selected_block\n",
        "\n",
        "    def prune_and_regrow(self):\n",
        "        \"\"\"Apply pruning and regrowth only to the selected block.\"\"\"\n",
        "        for name, module in self.model.named_modules():\n",
        "            if name == self.selected_block_name:\n",
        "                self.masking.prune_rate = self.prune_rate\n",
        "                self.masking.step()\n",
        "                self.masking.apply_mask()\n",
        "\n",
        "    def share_weights(self, enable_sharing=True):\n",
        "        \"\"\"Share only the sparsity pattern from the selected block to matching layers.\"\"\"\n",
        "        if not enable_sharing:\n",
        "            self.logger.info(\"Weight sharing is disabled.\")\n",
        "            return\n",
        "\n",
        "        source_block = next((module for name, module in self.model.named_modules() if name == self.selected_block_name), None)\n",
        "        if source_block is None or not hasattr(source_block, 'weight'):\n",
        "            self.logger.error(f\"No source block with weights found for {self.selected_block_name}.\")\n",
        "            return\n",
        "\n",
        "        source_mask = source_block.weight.data != 0\n",
        "\n",
        "        shared = False\n",
        "        for target_name, target_module in self.model.named_modules():\n",
        "            if hasattr(target_module, 'weight') and target_name != self.selected_block_name and 'shortcut' not in target_name:\n",
        "                if source_block.weight.size() == target_module.weight.size():\n",
        "                    # Apply the sparsity pattern\n",
        "                    target_module.weight.data *= source_mask.float()  # Apply mask\n",
        "                    shared = True\n",
        "                    self.logger.info(f\"Sparsity pattern shared from {self.selected_block_name} to {target_name}.\")\n",
        "\n",
        "        if not shared:\n",
        "            self.logger.info(\"No patterns were shared due to dimensional mismatch or other conditions.\")\n",
        "\n",
        "    def _share_sparsity_pattern(self, source_module, target_module):\n",
        "        \"\"\"Helper function to share the sparsity pattern from source to target module.\"\"\"\n",
        "        sparsity_mask = source_module.weight.data != 0\n",
        "        target_module.weight.data *= sparsity_mask.float()\n",
        "\n",
        "    def log_non_zero_weights(self, phase):\n",
        "        \"\"\"Log the percentage of non-zero weights for Conv2d layers after training.\"\"\"\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                total_weights = module.weight.data.numel()\n",
        "                non_zero_weights = torch.count_nonzero(module.weight.data).item()\n",
        "                percentage_non_zero = non_zero_weights / total_weights * 100\n",
        "                self.logger.info(f\"{phase} - {name}: {percentage_non_zero:.2f}% non-zero weights.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lprv9U47B2JO"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, device, epoch, selective_pruning):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        selective_pruning.apply_masks()  # Apply sparsity masks globally\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f'Train Epoch: {epoch+1} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_accuracy = 100. * correct / total\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    logging.info(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)')\n",
        "    return accuracy\n",
        "\n",
        "def validate(model, validate_loader, device):\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in validate_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            validation_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    validation_loss /= len(validate_loader)\n",
        "    validation_accuracy = 100. * correct / total\n",
        "    return validation_loss, validation_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K91aPe3aB4lY"
      },
      "outputs": [],
      "source": [
        "def select_model(model_name, num_classes):\n",
        "    if model_name.startswith('resnet'):\n",
        "        if model_name == 'resnet34':\n",
        "            return ResNet34(c=num_classes)\n",
        "        elif model_name == 'resnet50':\n",
        "            return ResNet50(c=num_classes)\n",
        "    elif model_name == 'vgg16':\n",
        "        return VGG16(config='D', num_classes=num_classes)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEtTz76CB-Ew"
      },
      "outputs": [],
      "source": [
        "def running_in_notebook():\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        return True\n",
        "    if any('SPYDER' in name for name in os.environ):\n",
        "        return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEQDIBsoB7IV"
      },
      "outputs": [],
      "source": [
        "def parse_args():\n",
        "    if running_in_notebook():\n",
        "        print(\"Running in a Jupyter notebook or IPython environment.\")\n",
        "        class Args:\n",
        "            epochs = 50\n",
        "            batch_size = 128\n",
        "            density = 0.5\n",
        "            prune_rate = 0.5\n",
        "            sparsity_ratio = 0.5\n",
        "            dataset = 'cifar10'\n",
        "            model = 'resnet34'\n",
        "            lr = 0.1  # Default learning rate\n",
        "            l2 = 5.0e-4  # Default L2 regularization strength\n",
        "            disable_weight_sharing = False\n",
        "        return Args()\n",
        "    else:\n",
        "        parser = argparse.ArgumentParser(description='Train a model with options.')\n",
        "        parser.add_argument('--epochs', type=int, default=50, help='Number of training epochs.')\n",
        "        parser.add_argument('--batch_size', type=int, default=128, help='Training batch size.')\n",
        "        parser.add_argument('--density', type=float, default=0.5, help='Density of the network connections.')\n",
        "        parser.add_argument('--prune_rate', type=float, default=0.5, help='Pruning rate for the network.')\n",
        "        parser.add_argument('--sparsity_ratio', type=float, default=0.1, help='Sparsity ratio used in selective pruning.')\n",
        "        parser.add_argument('--dataset', type=str, default='cifar10', choices=['cifar10'], help='Dataset to use (cifar10)')\n",
        "        parser.add_argument('--model', type=str, default='resnet34', choices=[ 'resnet34', 'resnet50', 'vgg16'], help='Model to use (resnet18, resnet34, resnet50, vgg16)')\n",
        "        parser.add_argument('--lr', type=float, default=0.1, help='Learning rate for the optimizer.')\n",
        "        parser.add_argument('--l2', type=float, default=5.0e-4, help='L2 weight decay for regularization.')\n",
        "        parser.add_argument('--disable_weight_sharing', action='store_true', help='Disable the weight sharing feature.')\n",
        "        return parser.parse_args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KETb0ElKB-rK"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    args = parse_args()\n",
        "    print(f\"Training with settings: epochs={args.epochs}, batch_size={args.batch_size}, density={args.density}, \"\n",
        "          f\"prune_rate={args.prune_rate}, sparsity_ratio={args.sparsity_ratio}, dataset={args.dataset}, disable_weight_sharing={args.disable_weight_sharing}\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    num_classes = {'cifar10': 10}.get(args.dataset, 10)\n",
        "    model = select_model(args.model, num_classes).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    scheduler = StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    train_loader, validate_loader, test_loader = load_dataset(args.dataset, args.batch_size)\n",
        "\n",
        "    selective_pruning = SelectivePruning(model, optimizer, args.density, args.prune_rate, args.sparsity_ratio)\n",
        "    selective_pruning.log_non_zero_weights('Before Training')\n",
        "    final_accuracy = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, device, epoch, selective_pruning)\n",
        "        validation_loss, validation_accuracy = validate(model, validate_loader, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        selective_pruning.prune_and_regrow()\n",
        "        # Log training and validation statistics for each epoch\n",
        "        logger.info(f'Epoch {epoch+1}: Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
        "        logger.info(f'Epoch {epoch+1}: Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.2f}%')\n",
        "\n",
        "        if not args.disable_weight_sharing:\n",
        "            selective_pruning.share_weights(True)\n",
        "        else:\n",
        "            selective_pruning.share_weights(False)\n",
        "        selective_pruning.log_non_zero_weights('After Training')\n",
        "\n",
        "        final_accuracy = evaluate(model, test_loader, device)\n",
        "        logger.info(f'Final Test Accuracy: {final_accuracy:.2f}%')\n",
        "        elapsed_time = time.time() - start_time  # End timing\n",
        "    print(f\"Total training time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtQAVRhJdawN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}